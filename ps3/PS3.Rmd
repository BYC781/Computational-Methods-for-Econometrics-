---
title: "PS3_program"
author: "Boyu Chen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
library(invgamma)
library(kableExtra)
library(optimx)
setwd("/Users/bychen/Documents/ECON-7218/ps3")
```

## Read Data

### group data

```{r readdata}
data_list <- list()
for (i in 1:76) {
  filename <- paste0("./problem_set_3_sample/group/group", i, ".dat")
  data <- read.table(filename)
  data_list[[i]] <- data
}
names(data_list) <- paste0("g", 1:76)
```

### network data
```{r readnetwork}
W <- list()
for (i in 1:76) {
  filename <- paste0("./problem_set_3_sample/network/network", i, ".dat")
  data <- read.table(filename)
  W[[i]] <- as.matrix(data)
}
names(W) <- paste0("W", 1:76)
```

## Set N, X, Y
```{r N}
N <- sapply(1:76, function(i){nrow(W[[i]])})
```
```{r Y X}
X <- list()
Y <- list()
for (i in 1:76) {
  X[[i]] <- data_list[[i]][, 1:17] %>% as.matrix()
  Y[[i]] <- data_list[[i]][, 18] %>% as.matrix()
}

```

```{r setcolname}
col_names <- c('age', 'male', 'black', 'asian', 'hisp', 'race.other', 
               'both.par', 'less.hs', 'more.hs', 'mom.edu.mis', 'welfare',
               'mom.job.miss', 'prof', 'job.other', 'sport', 'white', 
               'yr.school', 'gpa', 'overage')
```
```{r set initparam}
init.lambda <- 0.029
init.a.g <- rep(4.0, 76)
init.b <- c(-0.193, -0.111, -0.096, -0.012, 0.032, -0.117, 0.070, -0.145, 
            0.145, -0.081, 0.125, -0.042, -0.004, 0.029, 0.2, 0.3, 0.3,
            -0.005, -0.0029, -0.029, 0.040, -0.044, -0.076, 0.014, -0.04, 
            0.005, 0.038, 0.028, -0.007, 0.018, 0.046, 0.02, 0.03, 0.03)
init.sigma2 <- 0.459
init.param <- c(init.lambda, init.a.g, init.b, init.sigma2)
```

### Objective function

```{r loglikelihood}
obj.fn <- function(param){
    lambda <- param[1]
    a <- param[2:77]
    b <- param[78:111]
    sigma2 <- param[112]
    const <- -sum(N)/2 * log(2*pi)
    sigma2term <- - log(sigma2) * sum(N)/2 # OK
    
    logdetterm <-  sum(sapply(1:76, function(g) {
        logdetterm <- log(det(diag(N[g]) - lambda * W[[g]]))
        return(logdetterm)
    })) # OK
    
    inprodterm <- -(2*sigma2)^-1 * sum(sapply(1:76, function(g) {
        S.g <- diag(N[g]) - lambda * as.matrix(W[[g]])
        Y.g <- Y[[g]]
        XX.g <- cbind(X[[g]], W[[g]] %*% X[[g]])
        l.g <- rep(1, N[g])
        e.g <- S.g %*% Y.g - XX.g %*% b - a[g] * l.g
        e.gT_e.g <- t(e.g) %*% e.g
        return(e.gT_e.g)
    }))

    likelihood <- const + sigma2term + logdetterm + inprodterm
    return(- likelihood)
}
```

### Run MLE, using "L-BFGS-B" and set bounds for lambda
```{r runMLE}
result <-  optim(par = init.param, fn = obj.fn, method = "L-BFGS-B", 
                 lower = c(-0.1, rep(-Inf, 111)), 
                 upper = c(0.1, rep(Inf, 111)), control = list(maxit = 10000), 
                 hessian = FALSE)
```

```{r for hess}
result <-  optim(par = result$par, fn = obj.fn, method = "L-BFGS-B", 
                 lower = c(-0.1, rep(-Inf, 111)), 
                 upper = c(0.1, rep(Inf, 111)), control = list(maxit = 1), 
                 hessian = TRUE)
```

```{r MLE result}
mledf <- data.frame(
    var.name = c("lambda", paste0("a", 1:76), 
                 paste0("b1.", 1:17), paste0("b2.", 1:17), "sigma2"),
    estimation = result$par,
    std.error = sqrt(diag(solve(result$hessian)))
    )
kable(mledf)
```

```{r}
write.csv(mledf, file="mle.csv")
```

# MCMC

## Set init.param
```{r setMCMC}
T = 5000 # number of iterations during Markov process
# assign param in prior distribution
# beta k = 17 so 2k = 34
b_0 <- rep(0, 34)
B_0 <- diag(3, 34, 34)
# alpha
a_0 <- 1; A_0 <- 0.5
# sigma2
k_0 <- 3; v_0 <- 3
lambda_prime = 0


# saver
lambda_T <- rep(NA, T)
beta_T <- matrix(nrow = T, ncol = 34)
alpha_T <- matrix(nrow = T, ncol = 76)
sigma2_T <- rep(NA, T)
```

```{r whent=1}
# starting value of draw
tao_G = sapply(1:76, function(g){W[[g]] %>% rowSums() %>% max()}) %>% max()
lambda_T[1] <- runif(1, -1/tao_G, 1/tao_G)
beta_T[1,] <- mvrnorm(1, mu=b_0, Sigma=B_0)
alpha_T[1,] <- rnorm(76, a_0, A_0)
sigma2_T[1] <- rinvgamma(1, k_0/2, v_0/2)
```

## Main Process

```{r M-H within Gibbs}
# propose lambda*
for (t in 2:T){
    accept = 0
    while (accept == 0){
        if (t<=2){
            lambda_prime <- mvrnorm(mu = lambda_T[t-1], Sigma = 0.1^2 * diag(1))
        }else{
            lambda_prime <- mvrnorm(mu = lambda_T[t-1], 
                                    Sigma = 2.38^2 * var(lambda_T[1:t-1])) * 0.95 +
                            mvrnorm(mu = lambda_T[t-1], Sigma = 0.1^2 * diag(1)) * 0.05
        }
        accept = ifelse(lambda_prime >= -1/tao_G && lambda_prime <= 1/tao_G, 1, 0)
    }
    lambda_T[t] <- lambda_prime
    
    pp_l = 0
    for (g in 1:76){
        S_1 <- diag(N[g]) - lambda_prime * W[[g]] %>% as.matrix()
        S_2 <- diag(N[g]) - lambda_T[t-1] * W[[g]] %>% as.matrix()
        XX.g <- cbind(X[[g]], W[[g]]%*%X[[g]])
        ep_1 <- S_1 %*% Y[[g]] - XX.g %*% beta_T[t-1, ] - rep(1, N[g]) * alpha_T[t-1, g]
        ep_2 <- S_2 %*% Y[[g]] - XX.g %*% beta_T[t-1, ] - rep(1, N[g]) * alpha_T[t-1, g]
        V = sigma2_T[t-1] * diag(N[g])
        like_1 <- log(det(S_1)) - 0.5 * t(ep_1) %*% solve(V) %*% ep_1
        like_2 <- log(det(S_2)) - 0.5 * t(ep_2) %*% solve(V) %*% ep_2
        pp_l = pp_l + like_1 - like_2
    }
    pp_l = min(c(exp(pp_l), 1))
    lambda_T[t] = ifelse(runif(1) <= pp_l, lambda_prime, lambda_T[t-1])
    
    # THE SAMPLING OF BETA FROM PROSTERIOR DISTRIBUTION
    B <- lapply(1:76, function(g){
        XX.g <- cbind(X[[g]], W[[g]]%*%X[[g]])
        return(t(XX.g) %*% XX.g)
    })
    B <- Reduce("+", B)
    B <- solve(solve(B_0) + B)
    
    beta_hat <- lapply(1:76, function(g){
        sigma2 <- sigma2_T[t-1]
        S.g <- diag(N[g]) - lambda_T[t] * W[[g]] %>% as.matrix()
        XX.g <- cbind(X[[g]], W[[g]]%*%X[[g]])
        Y.g <- Y[[g]]
        l.g <- rep(1, N[g])
        a.g <- alpha_T[t-1]
        return(sigma2^-1 * t(XX.g) %*% (S.g%*%Y.g - l.g*a.g))
    })
    beta_hat <- Reduce("+", beta_hat)
    beta_hat <- B %*% (solve(B_0)%*%b_0 + beta_hat)
    
    beta_T[t,] <- mvrnorm(n=1, mu=beta_hat, Sigma=B)
    
    # THE SAMPLING OF SIGMA_E^2 FROM PROSTERIOR DISTRIBUTION
    sum_ep.gTep.g <- sapply(1:76, function(g){
        S.g <- diag(N[g]) - lambda_T[t] * W[[g]] %>% as.matrix()
        Y.g <- Y[[g]]
        XX.g <- cbind(X[[g]], W[[g]]%*%X[[g]])
        l.g <- rep(1, N[g])
        a.g <- alpha_T[t-1]
        beta <- beta_T[t,]
        ep.g <- S.g %*% Y.g - XX.g %*% beta - l.g*a.g
        return(t(ep.g)%*%ep.g)
    }) %>% sum()

    sigma2_T[t] <- rinvgamma(1, (k_0 + sum(N))/2, (v_0+ sum_ep.gTep.g)/2)
    
    # THE SAMPLING OF ALPHA_G FROM PROSTERIOR DISTRIBUTION
    sigma2 <- sigma2_T[t]
    l.g <- rep(1, N[g])
    S.g <- diag(N[g]) - lambda_T[t] * W[[g]] %>% as.matrix()
    Y.g <- Y[[g]]
    beta <- beta_T[t,]
    cbind(X[[g]], W[[g]]%*%X[[g]])
    R.g <- (A_0^-1 + sigma2^-1 * t(l.g) %*% l.g) ^-1
    a.g_hat <- R.g * (A_0^-1 * a_0 + sigma2^-1 * t(l.g)%*%(S.g%*%Y.g-XX.g%*%beta))
    alpha_T[t, ] <- rnorm(76, a.g_hat, R.g)
}
```


## Posterior Results

```{r posterior result}
posterior_result <- list()
posterior_result$alpha_mean <- sapply(1:76, function(g){mean(alpha_T[,g])})
posterior_result$alpha_sd <- sapply(1:76, function(g){sd(alpha_T[,g])})
posterior_result$beta_mean <- sapply(1:34, function(k){mean(beta_T[,k])})
posterior_result$beta_sd <- sapply(1:34, function(k){sd(beta_T[,k])})
posterior_result$lambda_mean <- mean(lambda_T)
posterior_result$lambda_sd <- sd(lambda_T)
posterior_result$sigma2_mean <- mean(sigma2_T)
posterior_result$sigma2_sd <- sd(sigma2_T)
```
```{r posterior df}
var.name <- c("lambda", paste0("b1.", 1:17), paste0("b2.", 1:17), paste0("a", 1:76), "sigma2")
var.mean <- c(posterior_result$lambda_mean, posterior_result$beta_mean, 
          posterior_result$alpha_mean, posterior_result$lambda_mean)
var.sd <- c(posterior_result$lambda_sd, posterior_result$beta_sd, 
          posterior_result$alpha_sd, posterior_result$lambda_sd)
df <- data.frame(cbind(var.name, var.mean, var.sd))
kable(df)
```
```{r write}
write_csv(df, file = "mcmc.csv")
```


